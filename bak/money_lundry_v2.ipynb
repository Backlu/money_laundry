{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28afaf78",
   "metadata": {},
   "source": [
    "# 洗錢\n",
    "- 筆記: https://www.notion.so/jayschsu/c6a6219dd004469bbbfbecfb6d9883f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e6b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, RandomOverSampler, ADASYN, KMeansSMOTE, SMOTEN, SMOTENC, SVMSMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from collections import namedtuple\n",
    "import joblib\n",
    "\n",
    "SCORE_TUPLE = namedtuple('SCORE_TUPLE', 'feature, preprocess, sampling, model, iter, precision, time')\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c0c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Data\n",
    "from log import init_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "963b3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_logging('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f0ec1cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m2022-10-31 09:14:03,689 INFO: func:init_data: 2.66 sec (utils.py:16)\u001b[0m\n",
      "\u001b[38;20m2022-10-31 09:14:03,690 INFO: custinfo_preprocess (data.py:52)\u001b[0m\n",
      "invalid_data_qty: 2\n",
      "\u001b[38;20m2022-10-31 09:14:03,799 INFO: func:custinfo_preprocess: 0.11 sec (utils.py:16)\u001b[0m\n",
      "\u001b[38;20m2022-10-31 09:14:03,800 INFO: featuring_alertDate (data.py:81)\u001b[0m\n",
      "\u001b[38;20m2022-10-31 09:14:29,345 INFO: func:featuring_alertDate: 25.55 sec (utils.py:16)\u001b[0m\n",
      "\u001b[38;20m2022-10-31 09:14:29,346 INFO: featuring_accumulate (data.py:147)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   4 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-2)]: Done  11 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-2)]: Done  18 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-2)]: Batch computation too fast (0.1918s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-2)]: Done  27 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-2)]: Done  37 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-2)]: Batch computation too fast (0.0439s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-2)]: Done  59 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-2)]: Batch computation too fast (0.0872s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-2)]: Done  99 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-2)]: Batch computation too fast (0.1565s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=-2)]: Done 183 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-2)]: Done 343 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-2)]: Done 583 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-2)]: Done 823 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-2)]: Done 1095 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-2)]: Done 1367 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-2)]: Done 1671 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-2)]: Done 1975 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-2)]: Done 2311 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-2)]: Done 2647 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-2)]: Done 3015 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-2)]: Done 3383 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-2)]: Done 3783 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-2)]: Done 3945 out of 3945 | elapsed:   10.5s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Batch computation too fast (0.0152s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-2)]: Done   4 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-2)]: Done  11 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-2)]: Batch computation too fast (0.0519s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-2)]: Done  22 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-2)]: Done  40 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-2)]: Batch computation too fast (0.0891s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-2)]: Done  74 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-2)]: Batch computation too fast (0.1546s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=-2)]: Done 138 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-2)]: Done 242 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-2)]: Done 450 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-2)]: Done 658 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-2)]: Done 898 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-2)]: Done 1138 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-2)]: Done 1410 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-2)]: Done 1682 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-2)]: Done 1986 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-2)]: Done 2290 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-2)]: Done 2626 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-2)]: Done 2962 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-2)]: Done 3330 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=-2)]: Done 3698 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-2)]: Done 4098 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-2)]: Done 4498 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-2)]: Done 4930 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-2)]: Done 5362 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=-2)]: Done 5826 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-2)]: Done 6095 tasks      | elapsed:   17.8s\n",
      "[Parallel(n_jobs=-2)]: Done 6196 out of 6196 | elapsed:   17.9s finished\n",
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Batch computation too fast (0.0136s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-2)]: Done   4 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-2)]: Done  11 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-2)]: Batch computation too fast (0.0431s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-2)]: Done  22 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-2)]: Done  40 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-2)]: Batch computation too fast (0.0650s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-2)]: Done  74 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-2)]: Batch computation too fast (0.1232s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=-2)]: Done 138 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-2)]: Done 242 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-2)]: Done 450 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-2)]: Done 658 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-2)]: Done 898 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-2)]: Done 1033 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-2)]: Done 1095 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-2)]: Done 1144 out of 1144 | elapsed:    2.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m2022-10-31 09:15:08,912 INFO: func:featuring_accumulate: 39.57 sec (utils.py:16)\u001b[0m\n",
      "\u001b[38;20m2022-10-31 09:15:08,914 INFO: featuring_integrate (data.py:162)\u001b[0m\n",
      "\u001b[38;20m2022-10-31 09:15:30,572 INFO: func:featuring_integrate: 21.66 sec (utils.py:16)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927123e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b499f29",
   "metadata": {},
   "source": [
    "### Data\n",
    "- dp_df\n",
    "    - DEBIT是進錢(+) & CREDIT是出錢(-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9592f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccba_df = pd.read_csv('data/dataset1/public_train_x_ccba_full_hashed.csv')\n",
    "cdtx_df = pd.read_csv('data/dataset1/public_train_x_cdtx0001_full_hashed.csv')\n",
    "custinfo_df = pd.read_csv('data/dataset1/public_train_x_custinfo_full_hashed.csv')\n",
    "dp_df = pd.read_csv('data/dataset1/public_train_x_dp_full_hashed.csv')\n",
    "remit_df = pd.read_csv('data/dataset1/public_train_x_remit1_full_hashed.csv')\n",
    "tr_alertX_df = pd.read_csv('data/dataset1/train_x_alert_date.csv')\n",
    "tr_sarY_df = pd.read_csv('data/dataset1/train_y_answer.csv')\n",
    "public_alertX_df = pd.read_csv('data/dataset1/public_x_alert_date.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b08468",
   "metadata": {},
   "source": [
    "**Preprocess**\n",
    "- 將alert_date, sar_flag merge到custinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aba213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping alert_date & sar flag\n",
    "alertDate_dict = tr_alertX_df.set_index('alert_key').to_dict()['date']\n",
    "sarFlag_dict = tr_sarY_df.set_index('alert_key').to_dict()['sar_flag']\n",
    "public_alertX_df['sar_flag']=2\n",
    "alertDatePublc_dict = public_alertX_df.set_index('alert_key').to_dict()['date']\n",
    "sarFlagPublic_dict = public_alertX_df.set_index('alert_key').to_dict()['sar_flag']\n",
    "custinfo_df['alert_date'] = custinfo_df['alert_key'].map(lambda x: alertDate_dict[x] if x in alertDate_dict else alertDatePublc_dict[x] )\n",
    "custinfo_df['sar_flag'] = custinfo_df['alert_key'].map(lambda x: sarFlag_dict[x] if x in sarFlag_dict else sarFlagPublic_dict[x])\n",
    "custinfo_df['sar_flag_nunique'] = custinfo_df.groupby(['cust_id','alert_date'])['sar_flag'].transform('nunique')\n",
    "invalid_data_qty = sum(custinfo_df['sar_flag_nunique']>1)\n",
    "print(f'invalid_data_qty: {invalid_data_qty}')\n",
    "custinfo_valid = custinfo_df[custinfo_df['sar_flag_nunique']==1]\n",
    "custinfo_valid.drop_duplicates(['cust_id','alert_date','sar_flag'], inplace=True)\n",
    "custinfo_valid = custinfo_valid.set_index(['cust_id','alert_date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db39ff8f",
   "metadata": {},
   "source": [
    "## Feturing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db996e9",
   "metadata": {},
   "source": [
    "**1. by Date**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5508c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cdtx\n",
    "cdtxDate_ = cdtx_df.groupby(['cust_id','date']).agg(cdtxAmt_ADate=('amt', sum), cdtxCnt_ADate=('date', 'count'))\n",
    "cdtxDate_ntd = cdtx_df[cdtx_df['cur_type']==47].groupby(['cust_id','date']).agg(cdtxAmtNTD_ADate=('amt', sum), cdtxCntNTD_ADate=('date', 'count'))\n",
    "cdtxDate_fc = cdtx_df[cdtx_df['cur_type']!=47].groupby(['cust_id','date']).agg(cdtxAmtFC_ADate=('amt', sum), cdtxCntFC_ADate=('date', 'count'))\n",
    "cdtxDate_tw = cdtx_df[cdtx_df['country']==130].groupby(['cust_id','date']).agg(cdtxAmtTW_ADate=('amt', sum), cdtxCntTW_ADate=('date', 'count'))\n",
    "cdtxDate_f = cdtx_df[cdtx_df['country']!=130].groupby(['cust_id','date']).agg(cdtxAmtF_ADate=('amt', sum), cdtxCntF_ADate=('date', 'count'))\n",
    "cdtxDate_df = pd.concat([cdtxDate_, cdtxDate_ntd, cdtxDate_fc, cdtxDate_tw, cdtxDate_f], axis=1)\n",
    "\n",
    "#dp\n",
    "dp_df['amt'] = dp_df['tx_amt']*dp_df['exchg_rate']\n",
    "dpDate_ = dp_df.groupby(['cust_id','tx_date']).agg(dpAmt_ADate=('tx_amt', sum), dpCnt_ADate=('tx_date', 'count'))\n",
    "dpDate_CR = dp_df[dp_df['debit_credit']=='CR'].groupby(['cust_id','tx_date']).agg(dpAmtCR_ADate=('tx_amt', sum), dpCntCR_ADate=('tx_date', 'count'))\n",
    "dpDate_DB = dp_df[dp_df['debit_credit']=='CB'].groupby(['cust_id','tx_date']).agg(dpAmtDB_ADate=('tx_amt', sum), dpCntDB_ADate=('tx_date', 'count'))\n",
    "dpDate_CC = dp_df[(dp_df['tx_type']==1)&(dp_df['info_asset_code']==12)].groupby(['cust_id','tx_date']).agg(dpAmtCC_ADate=('tx_amt', sum), dpCntCC_ADate=('tx_date', 'count'))\n",
    "dpDate_NCC = dp_df[~((dp_df['tx_type']==1)&(dp_df['info_asset_code']==12))].groupby(['cust_id','tx_date']).agg(dpAmtNCC_ADate=('tx_amt', sum), dpCntNCC_ADate=('tx_date', 'count'))\n",
    "dpDate_CBank = dp_df[dp_df['cross_bank']==1].groupby(['cust_id','tx_date']).agg(dpAmtCBank_ADate=('tx_amt', sum), dpCntCBank_ADate=('tx_date', 'count'))\n",
    "dpDate_InBank = dp_df[dp_df['cross_bank']==0].groupby(['cust_id','tx_date']).agg(dpAmtInBank_ADate=('tx_amt', sum), dpCntInBank_ADate=('tx_date', 'count'))\n",
    "dpDate_ATM = dp_df[dp_df['ATM']==1].groupby(['cust_id','tx_date']).agg(dpAmtATM_ADate=('tx_amt', sum), dpCntATM_ADate=('tx_date', 'count'))\n",
    "dpDate_NATM = dp_df[dp_df['ATM']==0].groupby(['cust_id','tx_date']).agg(dpAmtNATM_ADate=('tx_amt', sum), dpCntNATM_ADate=('tx_date', 'count'))\n",
    "dpDate_branchNunique = dp_df.groupby(['cust_id','tx_date']).agg(dpBranchNunique_ADate=('txbranch', 'nunique'))\n",
    "dpDate_df = pd.concat([dpDate_, dpDate_CR, dpDate_DB, dpDate_CC, dpDate_NCC, dpDate_CBank, dpDate_InBank, dpDate_ATM, dpDate_NATM, dpDate_branchNunique], axis=1)\n",
    "\n",
    "#remit\n",
    "remitDate_df = remit_df.groupby(['cust_id','trans_date']).agg(remitAmt_ADate=('trade_amount_usd', sum), remitCnt_ADate=('trans_no', 'count'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dad98a",
   "metadata": {},
   "source": [
    "**2. Accumulate recent K days**   \n",
    "- accumulate feature: 整合前K天的feature\n",
    "- Process: 每一個customer要分別做以下的feature engineering\n",
    "    - 1. 組合cust_id * dates\n",
    "    - 2. merge with date feature\n",
    "    - 3. calculate accum feature\n",
    "    - 4. merge date and accum features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af975aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_accum_feature(cust_data, cust_id, date_colName, rolling_window):\n",
    "    dates = list(range(cust_data.index.min(), cust_data.index.max()+1))\n",
    "    all_custDate = pd.DataFrame(list(product([cust_id], dates)), columns=['cust_id', date_colName]).set_index(['cust_id', date_colName])\n",
    "    cust_fullDate = pd.merge(cust_data, all_custDate, how='outer', left_index=True, right_index=True)\n",
    "    cust_fullDate.sort_index(ascending=True, inplace=True)\n",
    "    cust_fullDate.fillna(0, inplace=True)\n",
    "    \n",
    "    cust_recentAccum = cust_fullDate.rolling(window=rolling_window, min_periods=1).mean().shift()\n",
    "    cust_recentAccum2W = cust_fullDate.rolling(window=rolling_window*2, min_periods=1).mean().shift()\n",
    "    cust_recentAccumDiff = cust_recentAccum*2 - cust_recentAccum2W\n",
    "    cust_fullDateDiff = cust_fullDate - cust_recentAccum\n",
    "    \n",
    "    new_cols = []\n",
    "    for c in cust_recentAccum.columns:\n",
    "        new_cols.append(c.replace('_ADate', '_Accum'))\n",
    "    cust_recentAccum.columns=new_cols\n",
    "    \n",
    "    new_cols = []\n",
    "    for c in cust_recentAccumDiff.columns:\n",
    "        new_cols.append(c.replace('_ADate', '_AccumDiff'))\n",
    "    cust_recentAccumDiff.columns=new_cols\n",
    "    \n",
    "    new_cols = []\n",
    "    for c in cust_fullDateDiff.columns:\n",
    "        new_cols.append(c.replace('_ADate', '_ADateDiff'))\n",
    "    cust_fullDateDiff.columns=new_cols        \n",
    "    \n",
    "    feature = pd.merge(cust_data, cust_fullDateDiff, how='left', left_index=True, right_index=True)\n",
    "    feature = pd.merge(feature, cust_recentAccum, how='left', left_index=True, right_index=True)\n",
    "    feature = pd.merge(feature, cust_recentAccumDiff, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    return feature\n",
    "\n",
    "def accum_featuring_parallel(dateFeature, date_colName='date', rolling_window=5):\n",
    "    idx_custid, _ = zip(*dateFeature.index)\n",
    "    idx_custid = np.unique(idx_custid)\n",
    "    accu_feature_list = Parallel(n_jobs=4, verbose=10)(delayed(get_accum_feature)(dateFeature.loc[(cust_id)], cust_id, date_colName, rolling_window) for cust_id in idx_custid) \n",
    "    accu_feature = pd.concat(accu_feature_list)\n",
    "    return accu_feature\n",
    "\n",
    "def get_recallN_Precision(y_predProb, y_true):\n",
    "    y_pred_df = pd.DataFrame(list(zip(y_predProb, y_true)), columns=['predProb','trueLabel'])\n",
    "    y_pred_df.sort_values(by='predProb', ascending=False, inplace=True)\n",
    "    y_pred_df['idx']=list(range(len(y_pred_df)))\n",
    "    idx = y_pred_df[y_pred_df['trueLabel']==1]['idx'].iloc[-2]\n",
    "    precision_score = (sum(y_true)-1)/idx\n",
    "    return precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2596054",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cdtx_feature = accum_featuring_parallel(cdtxDate_df, 'date', rolling_window=7)\n",
    "dp_feature = accum_featuring_parallel(dpDate_df, 'tx_date', rolling_window=7)\n",
    "remit_feature = accum_featuring_parallel(remitDate_df, 'trans_date', rolling_window=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9496c6",
   "metadata": {},
   "source": [
    "**3. cust profile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff2a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ccba\n",
    "ccbaProfile1 = ccba_df.replace(0, np.nan).groupby('cust_id').agg(\n",
    "                               ccbalupayAmt_Year=('lupay', np.mean),\\\n",
    "                               ccbausgamAmt_Year=('usgam', np.mean),\\\n",
    "                               ccbacycamAax_Year=('cycam', np.max),\\\n",
    "                               ccbaclamtAmt_Year=('clamt', np.mean),\\\n",
    "                               ccbacsamtAmt_Year=('csamt', np.mean),\\\n",
    "                               ccbainamtAmt_Year=('inamt', np.mean),\\\n",
    "                               ccbacucsmAmt_Year=('cucsm', np.mean),\\\n",
    "                               ccbacucahAmt_Year=('cucah', np.mean),\\\n",
    "                              )\n",
    "ccbaProfile2 = ccba_df.groupby('cust_id').agg(ccbalupayCnt_Year=('lupay', np.count_nonzero),\\\n",
    "                               ccbausgamCnt_Year=('usgam', np.count_nonzero),\\\n",
    "                               ccbaclamtCnt_Year=('clamt', np.count_nonzero),\\\n",
    "                               ccbacsamtCnt_Year=('csamt', np.count_nonzero),\\\n",
    "                               ccbainamtCnt_Year=('inamt', np.count_nonzero),\\\n",
    "                               ccbacucsmCnt_Year=('cucsm', np.count_nonzero),\\\n",
    "                               ccbacucahCnt_Year=('cucah', np.count_nonzero),\\\n",
    "                               ccbabyymmCnt_Year=('byymm', np.count_nonzero),\\\n",
    "                              )\n",
    "ccbaProfile_df = pd.concat([ccbaProfile1, ccbaProfile2], axis=1)\n",
    "\n",
    "\n",
    "new_cols = []\n",
    "for c in custinfo_valid.columns:\n",
    "    if c in ['alert_key', 'sar_flag', 'sar_flag_nunique']:\n",
    "        new_cols.append(c)\n",
    "    else:\n",
    "        new_cols.append(c+'_Profile')\n",
    "custinfo_valid.columns=new_cols\n",
    "\n",
    "custProfile_df = pd.merge(custinfo_valid.reset_index(level=1), ccbaProfile_df, left_index=True, right_index=True, how='left')\n",
    "custProfile_df.set_index('alert_date', append=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2584f1ab",
   "metadata": {},
   "source": [
    "**3. Integrate dataset**\n",
    "- moneyLundry dataset (sar_flag==1, in alert_date)\n",
    "- falseAlarm dataset (sar_flag==0, in alert_date)\n",
    "- public dataset (sar_flag==2, in alert_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33540a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat([custProfile_df, cdtx_feature, dp_feature, remit_feature], axis=1)\n",
    "data_df['sar_flag'] =  data_df.apply(lambda r: r['sar_flag'] if r['alert_key']==r['alert_key'] else 3, axis=1)\n",
    "data_df = data_df[data_df['alert_key'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a1f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelY = 'sar_flag'\n",
    "feature_all = [x for x in data_df.columns if x not in ['alert_key', 'sar_flag', 'sar_flag_nunique']]\n",
    "feature_Profile = [x for x in data_df.columns if x.split('_')[-1]=='Profile']\n",
    "feature_ADate = [x for x in data_df.columns if x.split('_')[-1]=='ADate']\n",
    "feature_ADateDiff = [x for x in data_df.columns if x.split('_')[-1]=='ADateDiff']\n",
    "feature_Accum = [x for x in data_df.columns if x.split('_')[-1]=='Accum']\n",
    "feature_AccumDiff = [x for x in data_df.columns if x.split('_')[-1]=='AccumDiff']\n",
    "feature_Year = [x for x in data_df.columns if x.split('_')[-1]=='Year']\n",
    "assert len(feature_all)==len(feature_Profile+feature_ADate+feature_ADateDiff+feature_Accum+feature_AccumDiff+feature_Year)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcbe82a",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(data_df[labelY].value_counts().to_frame())\n",
    "dataset_Tr = data_df[data_df['sar_flag']!=2]\n",
    "dataset_TsPub = data_df[data_df['sar_flag']==2]\n",
    "dataset_Tr.fillna(0, inplace=True)\n",
    "dataset_TsPub.fillna(0, inplace=True)\n",
    "print('dataset_Tr:', dataset_Tr.shape)\n",
    "print('dataset_TsPub:', dataset_TsPub.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33aa816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_add_hat(x, features):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    df = x.copy() \n",
    "    q95_dict = {} \n",
    "    q5_dict = {} \n",
    "    for col in features:\n",
    "        q95 = np.percentile(df[col], 90)\n",
    "        q5 = np.percentile(df[col], 5) \n",
    "        q95_dict[col] = q95 \n",
    "        q5_dict[col] = q5 \n",
    "        b = np.array(df[col])\n",
    "        c = list(map(lambda x: q95 if x > q95 else x, b))\n",
    "        c = list(map(lambda x: q5 if x < q5 else x, c))        \n",
    "        df = df.drop(col, axis=1)\n",
    "        df[col] = c \n",
    "    return df, q95_dict, q5_dict\n",
    "\n",
    "# 使用同一标准处理测试集\n",
    "def add_hat(x, features, q95_dict, q5_dict):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    df = x.copy()\n",
    "    len_d = len(df.index)  # 测试集大小\n",
    "    for col in features:\n",
    "        q95 = q95_dict[col]\n",
    "        q5 = q5_dict[col]\n",
    "        b = np.array(df[col])\n",
    "        c = list(map(lambda x:q95 if x > q95 else x, b))\n",
    "        c = list(map(lambda x: q5 if x < q5 else x, c))        \n",
    "        df = df.drop(col, axis=1)\n",
    "        df[col] = c\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92534eff",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eac6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, y_tr, y_val= train_test_split(dataset_Tr[featurePDAY_Diff], dataset_Tr[labelY].values, test_size=0.2, stratify=dataset_Tr[labelY])\n",
    "X_tr, q95_dict, q5_dict = train_add_hat(X_tr, featurePDAY_Diff)\n",
    "X_val = add_hat(X_val, featurePDAY_Diff, q95_dict, q5_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipePre = Pipeline([('sc', MinMaxScaler()), ('pca', PCA(n_components=0.999999999))])\n",
    "pipeSp = imbPipeline([('sp', RandomOverSampler(sampling_strategy=0.8))])\n",
    "\n",
    "x_tr_ = pipePre.fit_transform(X_tr)\n",
    "x_val_ = pipePre.transform(X_val.copy())\n",
    "x_re, y_re = pipeSp.fit_resample(x_tr_, y_tr)\n",
    "#x_re, y_re = x_tr_, y_tr\n",
    "model = LogisticRegression()\n",
    "model = model.fit(x_re, y_re)        \n",
    "#tr_score\n",
    "y_pred = model.predict(x_tr_)\n",
    "y_predProb = model.predict_proba(x_tr_)\n",
    "y_prob = np.max(y_predProb, axis=1)\n",
    "y_sarProb = [x[1] for x in y_predProb]\n",
    "tr_precision = get_recallN_Precision(y_sarProb, y_tr)\n",
    "tr_acc = accuracy_score(y_tr, y_pred)\n",
    "tr_recall = recall_score(y_tr, y_pred)\n",
    "\n",
    "#val_score\n",
    "y_predval = model.predict(x_val_)\n",
    "y_predProb = model.predict_proba(x_val_)\n",
    "y_predProb = [x[1] for x in y_predProb]\n",
    "precisionScore = get_recallN_Precision(y_predProb, y_val)\n",
    "score_tuple = SCORE_TUPLE('PDA', 'minmax', 'smote', 'LR', '', precisionScore,0)\n",
    "print(f'tr_acc: {tr_acc:.3f}, tr_precision:{tr_precision:.3f}, tr_recall:{tr_recall:.3f}')\n",
    "print(score_tuple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db989b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, y_tr, y_val= train_test_split(dataset_Tr[featurePDA], dataset_Tr[labelY].values, test_size=0.2, stratify=dataset_Tr[labelY])\n",
    "\n",
    "X_tr = X_tr[featurePDA]\n",
    "X_val = X_val[featurePDA]\n",
    "pipePre = Pipeline([('sc', MinMaxScaler()), ('pca', PCA(n_components=0.999999999))])\n",
    "pipeSp = imbPipeline([('sp', SMOTE())])\n",
    "\n",
    "x_tr_ = pipePre.fit_transform(X_tr)\n",
    "x_val_ = pipePre.transform(X_val.copy())\n",
    "x_re, y_re = pipeSp.fit_resample(x_tr_, y_tr)\n",
    "model = LogisticRegression(max_iter=30)\n",
    "#model = SGDClassifier(loss='log', max_iter=10)\n",
    "#model = XGBClassifier(n_estimators=10, use_label_encoder=False, tree_method='gpu_hist', gpu_id=0, eval_metric='logloss')\n",
    "model = model.fit(x_re, y_re)        \n",
    "#tr_score\n",
    "y_pred = model.predict(x_tr_)\n",
    "y_predProb = model.predict_proba(x_tr_)\n",
    "y_prob = np.max(y_predProb, axis=1)\n",
    "y_sarProb = [x[1] for x in y_predProb]\n",
    "tr_precision = get_recallN_Precision(y_sarProb, y_tr)\n",
    "tr_acc = accuracy_score(y_tr, y_pred)\n",
    "tr_recall = recall_score(y_tr, y_pred)\n",
    "print(f'tr_acc: {tr_acc:.3f}, tr_precision:{tr_precision:.3f}, tr_recall:{tr_recall:.3f}')\n",
    "\n",
    "#val_score\n",
    "y_predProb = model.predict_proba(x_val_)\n",
    "y_predProb = [x[1] for x in y_predProb]\n",
    "precisionScore = get_recallN_Precision(y_predProb, y_val)\n",
    "score_tuple = SCORE_TUPLE(featureName, pipePreName, pipeSpName, modelName, i, precisionScore,0)\n",
    "score_list.append(score_tuple)\n",
    "print(score_tuple)\n",
    "\n",
    "\n",
    "\n",
    "#過濾中間資料\n",
    "if True:\n",
    "    X_tr['y_sarProb']=y_sarProb\n",
    "    X_tr['y_tr']=y_tr\n",
    "    threshold = X_tr.sort_values(by='y_sarProb').iloc[int(len(X_tr)*0.95)]['y_sarProb']    \n",
    "    X_tr_new = X_tr[(X_tr['y_sarProb']<threshold) | (X_tr['y_tr']==1)]\n",
    "    y_tr_new = X_tr_new['y_tr'].values\n",
    "    del X_tr_new['y_sarProb']\n",
    "    del X_tr_new['y_tr']\n",
    "    del X_tr['y_sarProb']\n",
    "    del X_tr['y_tr']\n",
    "else:\n",
    "    X_tr_new = X_tr.copy()\n",
    "    \n",
    "x_tr_new = pipePre.fit_transform(X_tr_new)\n",
    "x_val_ = pipePre.transform(X_val.copy())\n",
    "x_re, y_re = pipeSp.fit_resample(x_tr_new, y_tr_new)\n",
    "model = LogisticRegression()\n",
    "#model = SGDClassifier(loss='log')\n",
    "#model = XGBClassifier(use_label_encoder=False, tree_method='gpu_hist', gpu_id=0, eval_metric='logloss')\n",
    "model = model.fit(x_re, y_re)        \n",
    "\n",
    "y_pred = model.predict(x_tr_new)\n",
    "y_predProb = model.predict_proba(x_tr_new)\n",
    "y_prob = np.max(y_predProb, axis=1)\n",
    "y_sarProb = [x[1] for x in y_predProb]\n",
    "tr_precision = get_recallN_Precision(y_sarProb, y_tr_new)\n",
    "tr_acc = accuracy_score(y_tr_new, y_pred)\n",
    "tr_recall = recall_score(y_tr_new, y_pred)\n",
    "\n",
    "\n",
    "#val_score\n",
    "y_predProb = model.predict_proba(x_val_)\n",
    "y_predProb = [x[1] for x in y_predProb]\n",
    "precisionScore = get_recallN_Precision(y_predProb, y_val)\n",
    "score_tuple = SCORE_TUPLE(featureName, pipePreName, pipeSpName, modelName, i, precisionScore,0)\n",
    "score_list.append(score_tuple)\n",
    "print(score_tuple, f'-- shape:{X_tr.shape}->{X_tr_new.shape}->{x_tr_new.shape}->{x_re.shape}')\n",
    "print(f'tr_acc: {tr_acc:.3f}, tr_precision:{tr_precision:.3f}, tr_recall:{tr_recall:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb0598",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd60c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipePreMinMax = Pipeline([('sc', MinMaxScaler()), ('pca', PCA(n_components=0.99999))])\n",
    "pipePreStd = Pipeline([('sc', StandardScaler()), ('pca', PCA(n_components=0.9))])\n",
    "pipePreMMStd = Pipeline([('sc1', MinMaxScaler()), ('sc2', StandardScaler()), ('pca', PCA(n_components=0.9))])\n",
    "#pipe_list = zip([pipePreMinMax, pipePreStd, pipePreMMStd], ['MinMaxScaler','StandardScaler','MinMaxScaler+StandardScaler'])\n",
    "pipe_list = zip([pipePreMinMax], ['MinMaxScaler'])\n",
    "pipe_list = list(pipe_list)\n",
    "\n",
    "pipeSpAdasyn = imbPipeline([('sp', ADASYN())])\n",
    "pipeSpSmoten = imbPipeline([('sp', SMOTEN())])\n",
    "pipeSpSvmsmote = imbPipeline([('sp', SVMSMOTE())])\n",
    "pipeSpRandom = imbPipeline([('sp', RandomOverSampler())])\n",
    "pipeSpSmote = imbPipeline([('sp', SMOTE())])\n",
    "pipeSpBSmote = imbPipeline([('sp', BorderlineSMOTE())])\n",
    "pipeSpTom = imbPipeline([('sp', TomekLinks())])\n",
    "pipeSpSmoteTom = imbPipeline([('sp1', BorderlineSMOTE()), ('sp2', TomekLinks())])\n",
    "sp_list = zip([pipeSpAdasyn, pipeSpSmoten, pipeSpSvmsmote, pipeSpRandom, pipeSpSmote, pipeSpBSmote, pipeSpTom, pipeSpSmoteTom], ['adaysn', 'Smoten', 'Svmsmote', 'random', 'smote','bsmote', 'tom', 'SmoteTom'])\n",
    "#sp_list = zip([pipeSpSmote],['smote'])\n",
    "sp_list = list(sp_list)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(3)\n",
    "svc = SVC(kernel=\"rbf\", C=0.025, probability=True)\n",
    "nusvc = NuSVC(probability=True)\n",
    "dtree = DecisionTreeClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "adb = AdaBoostClassifier()\n",
    "gdc = GradientBoostingClassifier()\n",
    "gnb = GaussianNB()\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "sgdc = SGDClassifier(loss='log', max_iter=10)\n",
    "modelLR = LogisticRegression()\n",
    "modelXGBC = XGBClassifier(use_label_encoder=False, tree_method='gpu_hist', gpu_id=0, eval_metric='logloss')\n",
    "modelMLPC = MLPClassifier()\n",
    "#model_list = zip([qda, lda, gnb, gdc, adb, rfc, dtree, knn, modelLR, modelXGBC, modelMLPC], ['qda', 'lda', 'gnb', 'gdc', 'adb', 'rfc', 'dtree', 'knn', 'LogisticRegression', 'XGBClassifier', 'MLPClassifier'])\n",
    "model_list = zip([modelLR, sgdc], ['LR', 'SGDC'])\n",
    "model_list = list(model_list)\n",
    "\n",
    "featureD = feature_ADate\n",
    "featurePD = feature_Profile+feature_ADate\n",
    "featurePDA = feature_Profile+feature_ADate+feature_Accum\n",
    "featurePDAY = feature_Profile+feature_ADate+feature_Accum+feature_Year\n",
    "featureD_Diff = feature_ADateDiff\n",
    "featurePD_Diff = feature_Profile+feature_ADateDiff\n",
    "featurePDA_Diff = feature_Profile+feature_ADateDiff+feature_AccumDiff\n",
    "featurePDAY_Diff = feature_Profile+feature_ADateDiff+feature_AccumDiff+feature_Year\n",
    "featureD_Composite = feature_ADate+feature_ADateDiff\n",
    "featurePD_Composite = feature_Profile+feature_ADate+feature_ADateDiff\n",
    "featurePDA_Composite = feature_Profile+feature_ADate+feature_Accum+feature_ADateDiff+feature_AccumDiff\n",
    "featurePDAY_Composite = feature_Profile+feature_ADate+feature_Accum+feature_ADateDiff+feature_AccumDiff+feature_Year\n",
    "feature_list = [featureD, featurePD, featurePDA, featurePDAY, featureD_Diff, featurePD_Diff, featurePDA_Diff, featurePDAY_Diff, featureD_Composite, featurePD_Composite, featurePDA_Composite, featurePDAY_Composite]\n",
    "feature_list = zip(feature_list,['D', 'PD', 'PDA', 'PDAY', 'D_Diff', 'PD_Diff','PDA_Diff','PDAY_Diff','D_Composite','PD_Composite', 'PDA_Composite', 'PDAY_Composite'])\n",
    "#feature_list = zip([featurePDA], ['PDA'])\n",
    "feature_list = list(feature_list)\n",
    "\n",
    "score_list_tmp=[]\n",
    "for featureX, featureName in feature_list:\n",
    "    for i in range(4):\n",
    "        for pipePre, pipePreName in pipe_list:\n",
    "            for pipeSp, pipeSpName in sp_list:\n",
    "                for model, modelName in model_list:\n",
    "                    score_list_tmp.append(SCORE_TUPLE(featureName, pipePreName, pipeSpName, modelName, i, 0,0))\n",
    "print('hyperparameter combinations:',len(score_list_tmp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044cabb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_list = []\n",
    "Y = dataset_Tr[labelY]\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=43)\n",
    "skf.get_n_splits(dataset_Tr,Y)\n",
    "for i, (tr_idx, val_idx) in enumerate(skf.split(dataset_Tr, Y)):\n",
    "    predProb_list4 = []\n",
    "    for featureX, featureName in feature_list:\n",
    "        X_tr, X_val = dataset_Tr[featureX].iloc[tr_idx], dataset_Tr[featureX].iloc[val_idx]\n",
    "        y_tr, y_val = Y.iloc[tr_idx], Y.iloc[val_idx]\n",
    "        predProb_list3 = []\n",
    "        for pipePre, pipePreName in pipe_list:\n",
    "            predProb_list2 = []\n",
    "            for pipeSp, pipeSpName in sp_list:\n",
    "                predProb_list1 = []\n",
    "                for model, modelName in model_list:\n",
    "                    startTime = time.time() \n",
    "                    #preprocess\n",
    "                    x_tr_ = X_tr.copy()\n",
    "                    x_val_ = X_val.copy()\n",
    "                    x_tr_ = pipePre.fit_transform(x_tr_)\n",
    "                    x_val_ = pipePre.transform(x_val_)\n",
    "                    x_re, y_re = pipeSp.fit_resample(x_tr_, y_tr)\n",
    "                    model = model.fit(x_re, y_re)        \n",
    "                    \n",
    "                    #tr_score\n",
    "                    y_pred = model.predict(x_tr_)\n",
    "                    y_predProb = model.predict_proba(x_tr_)\n",
    "                    y_predProb = [x[1] for x in y_predProb]\n",
    "                    tr_precision = get_recallN_Precision(y_predProb, y_tr)\n",
    "                    tr_acc = accuracy_score(y_tr, y_pred)\n",
    "                    tr_recall = recall_score(y_tr, y_pred)\n",
    "                    \n",
    "                    #val score\n",
    "                    y_predProb = model.predict_proba(x_val_)\n",
    "                    y_predProb = [x[1] for x in y_predProb]\n",
    "                    precisionScore = get_recallN_Precision(y_predProb, y_val)\n",
    "                    t=time.time()-startTime\n",
    "                    score_tuple = SCORE_TUPLE(featureName, pipePreName, pipeSpName, modelName, i, precisionScore, t)\n",
    "                    score_list.append(score_tuple)\n",
    "                    predProb_list1.append(y_predProb)\n",
    "                    predProb_list2.append(y_predProb)\n",
    "                    predProb_list3.append(y_predProb)\n",
    "                    predProb_list4.append(y_predProb)\n",
    "                    print(score_tuple)\n",
    "                    print(f'-- shape:{X_tr.shape}->{x_tr_.shape}, time:{t:.1f}, ts score:{precisionScore:.4f}')\n",
    "                    print(f'-- tr_acc: {tr_acc:.2f}, tr_precision:{tr_precision:.4f}, tr_recall:{tr_recall:.3f}')\n",
    "\n",
    "                precisionScore = get_recallN_Precision(np.mean(predProb_list1, axis=0), y_val)\n",
    "                score_list.append(SCORE_TUPLE(featureName, pipePreName, pipeSpName, 'ensemble', i, precisionScore, t))\n",
    "            precisionScore = get_recallN_Precision(np.mean(predProb_list2, axis=0), y_val)\n",
    "            score_list.append(SCORE_TUPLE(featureName, pipePreName, 'ensemble', 'ensemble', i, precisionScore, t))\n",
    "        precisionScore = get_recallN_Precision(np.mean(predProb_list3, axis=0), y_val)\n",
    "        score_list.append(SCORE_TUPLE(featureName, 'ensemble', 'ensemble', 'ensemble', i, precisionScore, t))\n",
    "    precisionScore = get_recallN_Precision(np.mean(predProb_list4, axis=0), y_val)\n",
    "    score_list.append(SCORE_TUPLE('ensemble', 'ensemble', 'ensemble', 'ensemble', i, precisionScore, t))        \n",
    "                \n",
    "                \n",
    "score_df = pd.DataFrame(score_list)\n",
    "score_df.sort_values('precision', ascending=False, inplace=True)\n",
    "#joblib.dump(score_df, 'model_log/score_df.pkl')                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb559f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c26f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#display(score_df.sort_values('precision', ascending=False))\n",
    "display(score_df.groupby(['feature'])['precision'].mean().to_frame().sort_values('precision', ascending=False))\n",
    "#display(score_df.groupby(['model','iter'])['precision'].max().to_frame())\n",
    "#display(score_df.groupby(['iter','model'])['precision'].max().to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(score_df.groupby('feature')['precision'].mean().to_frame().sort_values('precision', ascending=False))\n",
    "display(score_df.groupby('sampling')['precision'].mean().to_frame().sort_values('precision', ascending=False))\n",
    "display(score_df.groupby('model')['precision'].mean().to_frame().sort_values('precision', ascending=False))\n",
    "display(score_df.groupby('iter')['precision'].mean().to_frame().sort_values('precision', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccb4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(score_df[score_df['model']=='LogisticRegression'].groupby('sampling')['precision'].mean().to_frame().sort_values('precision', ascending=False))\n",
    "display(score_df[score_df['model']=='LogisticRegression'].groupby('feature')['precision'].mean().to_frame().sort_values('precision', ascending=False))\n",
    "display(score_df[score_df['model']=='LogisticRegression'].groupby('model')['precision'].mean().to_frame().sort_values('precision', ascending=False))\n",
    "display(score_df[score_df['model']=='LogisticRegression'].groupby('iter')['precision'].mean().to_frame().sort_values('precision', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0cacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b190e541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d0ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90749f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adac69f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9bfc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
