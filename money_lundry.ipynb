{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98125b57",
   "metadata": {},
   "source": [
    "# 洗錢\n",
    "- 洗錢手法: http://www.fullgoal.com.cn/contents/2016/6/22-3bbe1d7f2c28489abfa1aaf2a1b919e7.html\n",
    "- 剔除異常數據: https://www.zhihu.com/question/399472607"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5141da3",
   "metadata": {},
   "source": [
    "- dp_df\n",
    "    - DEBIT是進錢(+) & CREDIT是出錢(-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4cea619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ceb13de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cust_data(data, cust_id):\n",
    "    cust_df = data[data['cust_id']==cust_id]\n",
    "    return cust_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c7e83a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccba_df = pd.read_csv('data/dataset1/public_train_x_ccba_full_hashed.csv')\n",
    "cdtx_df = pd.read_csv('data/dataset1/public_train_x_cdtx0001_full_hashed.csv')\n",
    "custinfo_df = pd.read_csv('data/dataset1/public_train_x_custinfo_full_hashed.csv')\n",
    "dp_df = pd.read_csv('data/dataset1/public_train_x_dp_full_hashed.csv')\n",
    "remit_df = pd.read_csv('data/dataset1/public_train_x_remit1_full_hashed.csv')\n",
    "tr_alertX_df = pd.read_csv('data/dataset1/train_x_alert_date.csv')\n",
    "tr_sarY_df = pd.read_csv('data/dataset1/train_y_answer.csv')\n",
    "public_alertX_df = pd.read_csv('data/dataset1/public_x_alert_date.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a57e9b",
   "metadata": {},
   "source": [
    "**Preprocess**\n",
    "- 將alert_date, sar_flag merge到custinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "277ee842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid_data_qty: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayhsu/Library/Python/3.8/lib/python/site-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#mapping alert_date & sar flag\n",
    "alertDate_dict = tr_alertX_df.set_index('alert_key').to_dict()['date']\n",
    "sarFlag_dict = tr_sarY_df.set_index('alert_key').to_dict()['sar_flag']\n",
    "public_alertX_df['sar_flag']=2\n",
    "alertDatePublc_dict = public_alertX_df.set_index('alert_key').to_dict()['date']\n",
    "sarFlagPublic_dict = public_alertX_df.set_index('alert_key').to_dict()['sar_flag']\n",
    "custinfo_df['alert_date'] = custinfo_df['alert_key'].map(lambda x: alertDate_dict[x] if x in alertDate_dict else alertDatePublc_dict[x] )\n",
    "custinfo_df['sar_flag'] = custinfo_df['alert_key'].map(lambda x: sarFlag_dict[x] if x in sarFlag_dict else sarFlagPublic_dict[x])\n",
    "custinfo_df['sar_flag_nunique'] = custinfo_df.groupby(['cust_id','alert_date'])['sar_flag'].transform('nunique')\n",
    "invalid_data_qty = sum(custinfo_df['sar_flag_nunique']>1)\n",
    "print(f'invalid_data_qty: {invalid_data_qty}')\n",
    "custinfo_valid = custinfo_df[custinfo_df['sar_flag_nunique']==1]\n",
    "custinfo_valid.drop_duplicates(['cust_id','alert_date','sar_flag'], inplace=True)\n",
    "custinfo_valid = custinfo_valid.set_index(['cust_id','alert_date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131626ba",
   "metadata": {},
   "source": [
    "## Feturing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38c2303",
   "metadata": {},
   "source": [
    "**1. by Date**\n",
    "1. 把各種資料表轉換為以date為單位的特徵\n",
    "- ccba: 信用卡相關資料, 以月為單位的資料, 缺少public dataset區間,不需處理\n",
    "    \n",
    "- cdtx: 消費細項\n",
    "    - 每日消費金額\n",
    "    - 每日消費次數\n",
    "    - 每日台幣消費金額\n",
    "    - 每日台幣消費次數   \n",
    "    - 每日外幣消費金額\n",
    "    - 每日外幣消費次數\n",
    "    - 每日國內消費金額\n",
    "    - 每日國內消費次數    \n",
    "    - 每日國外消費金額\n",
    "    - 每日國外消費次數\n",
    "- dp: 借貸: 金額需要乘上匯率\n",
    "    - 借貸金額\n",
    "    - 借貸次數\n",
    "    - CR金額\n",
    "    - CR次數\n",
    "    - DB金額\n",
    "    - DB次數\n",
    "    - 臨櫃現金交易金額\n",
    "    - 臨櫃現金交易次數\n",
    "    - 非臨櫃現金交易金額\n",
    "    - 非臨櫃現金交易次數 \n",
    "    - 跨行交易金額\n",
    "    - 跨行交易次數\n",
    "    - 非跨行交易金額\n",
    "    - 非跨行交易次數\n",
    "    - 實體ATM交易金額\n",
    "    - 實體ATM交易次數\n",
    "    - 非實體ATM交易金額\n",
    "    - 非實體ATM交易次數  \n",
    "    - 分行數量\n",
    "- remit\n",
    "    - 外匯金額\n",
    "    - 外匯次數    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9468a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cdtx\n",
    "cdtxDate_ = cdtx_df.groupby(['cust_id','date']).agg({'amt':sum, 'date':'count'}).rename(columns={'amt':'cdtx_amt', 'date':'cdtx_cnt'})\n",
    "cdtxDate_ntd = cdtx_df[cdtx_df['cur_type']==47].groupby(['cust_id','date']).agg({'amt':sum, 'date':'count'}).rename(columns={'amt':'cdtx_amtNTD', 'date':'cdtx_cntNTD'})\n",
    "cdtxDate_fc = cdtx_df[cdtx_df['cur_type']!=47].groupby(['cust_id','date']).agg({'amt':sum, 'date':'count'}).rename(columns={'amt':'cdtx_amtFC', 'date':'cdtx_cntFC'})\n",
    "cdtxDate_tw = cdtx_df[cdtx_df['country']==130].groupby(['cust_id','date']).agg({'amt':sum, 'date':'count'}).rename(columns={'amt':'cdtx_amtTW', 'date':'cdtx_cntTW'})\n",
    "cdtxDate_f = cdtx_df[cdtx_df['country']!=130].groupby(['cust_id','date']).agg({'amt':sum, 'date':'count'}).rename(columns={'amt':'cdtx_amtF', 'date':'cdtx_cntF'})\n",
    "cdtxDate_df = pd.concat([cdtxDate_, cdtxDate_ntd, cdtxDate_fc, cdtxDate_tw, cdtxDate_f], axis=1)\n",
    "\n",
    "#dp\n",
    "dp_df['amt'] = dp_df['tx_amt']*dp_df['exchg_rate']\n",
    "dpDate_ = dp_df.groupby(['cust_id','tx_date']).agg({'tx_amt':sum, 'tx_date':'count'}).rename(columns={'tx_amt':'dp_amt', 'tx_date':'dp_cnt'})\n",
    "dpDate_CR = dp_df[dp_df['debit_credit']=='CR'].groupby(['cust_id','tx_date']).agg({'tx_amt':sum, 'tx_date':'count'}).rename(columns={'tx_amt':'dp_amtCR', 'tx_date':'dp_cntCR'})\n",
    "dpDate_DB = dp_df[dp_df['debit_credit']=='DB'].groupby(['cust_id','tx_date']).agg({'tx_amt':sum, 'tx_date':'count'}).rename(columns={'tx_amt':'dp_amtDB', 'tx_date':'dp_cntDB'})\n",
    "dpDate_CC = dp_df[(dp_df['tx_type']==1)&(dp_df['info_asset_code']==12)].groupby(['cust_id','tx_date']).agg({'tx_amt':sum, 'tx_date':'count'}).rename(columns={'tx_amt':'dp_amtCC', 'tx_date':'dp_cntCC'})\n",
    "dpDate_NCC = dp_df[~((dp_df['tx_type']==1)&(dp_df['info_asset_code']==12))].groupby(['cust_id','tx_date']).agg({'tx_amt':sum, 'tx_date':'count'}).rename(columns={'tx_amt':'dp_amtNCC', 'tx_date':'dp_cntNCC'})\n",
    "dpDate_CBank = dp_df[dp_df['cross_bank']==1].groupby(['cust_id','tx_date']).agg({'tx_amt':sum, 'tx_date':'count'}).rename(columns={'tx_amt':'dp_amtCBank', 'tx_date':'dp_cntCBank'})\n",
    "dpDate_InBank = dp_df[dp_df['cross_bank']==0].groupby(['cust_id','tx_date']).agg({'tx_amt':sum, 'tx_date':'count'}).rename(columns={'tx_amt':'dp_amtInBank', 'tx_date':'dp_cntInBank'})\n",
    "dpDate_ATM = dp_df[dp_df['ATM']==1].groupby(['cust_id','tx_date']).agg({'tx_amt':sum, 'tx_date':'count'}).rename(columns={'tx_amt':'dp_amtATM', 'tx_date':'dp_cntATM'})\n",
    "dpDate_NATM = dp_df[dp_df['ATM']==0].groupby(['cust_id','tx_date']).agg({'tx_amt':sum, 'tx_date':'count'}).rename(columns={'tx_amt':'dp_amtNATM', 'tx_date':'dp_cntNATM'})\n",
    "dpDate_branchNunique = dp_df.groupby(['cust_id','tx_date']).agg({'txbranch':'nunique', }).rename(columns={'txbranch':'dp_branchNunique'})\n",
    "dpDate_df = pd.concat([dpDate_, dpDate_CR, dpDate_DB, dpDate_CC, dpDate_NCC, dpDate_CBank, dpDate_InBank, dpDate_ATM, dpDate_NATM, dpDate_branchNunique], axis=1)\n",
    "\n",
    "#remit\n",
    "remitDate_df = remit_df.groupby(['cust_id','trans_date']).agg({'trade_amount_usd':sum, 'trans_no':'count'}).rename(columns={'trade_amount_usd':'remit_amt', 'trans_no':'remit_cnt'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72c97b",
   "metadata": {},
   "source": [
    "**2. Accumulate recent K days**   \n",
    "- accumulate feature: 整合前K天的feature\n",
    "- Process: 每一個customer要分別做以下的feature engineering\n",
    "    - 1. 組合cust_id * dates\n",
    "    - 2. merge with date feature\n",
    "    - 3. calculate accum feature\n",
    "    - 4. merge date and accum features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea7eb2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accum_feature(cust_data, cust_id, date_colName, rolling_window):\n",
    "    dates = list(range(cust_data.index.min(), cust_data.index.max()+1))\n",
    "    all_custDate = pd.DataFrame(list(product([cust_id], dates)), columns=['cust_id', date_colName]).set_index(['cust_id', date_colName])\n",
    "    cust_fullDate = pd.merge(cust_data, all_custDate, how='outer', left_index=True, right_index=True)\n",
    "    cust_fullDate.sort_index(ascending=True, inplace=True)\n",
    "    cust_recentAccum = cust_fullDate.rolling(window = rolling_window).sum().shift()\n",
    "    new_cols = []\n",
    "    for c in cust_recentAccum.columns:\n",
    "        new_cols.append(c+'_Accum')\n",
    "    cust_recentAccum.columns=new_cols\n",
    "    feature = pd.merge(cust_data, cust_recentAccum, how='left', left_index=True, right_index=True)\n",
    "    return feature\n",
    "\n",
    "def accum_featuring(dateFeature, date_colName='date', rolling_window=5):\n",
    "    idx_custid, _ = zip(*dateFeature.index)\n",
    "    idx_custid = np.unique(idx_custid)\n",
    "    accu_feature_list = Parallel(n_jobs=4)(delayed(get_accum_feature)(dateFeature.loc[(cust_id)], cust_id, date_colName, rolling_window) for cust_id in idx_custid) \n",
    "    accu_feature = pd.concat(accu_feature_list)\n",
    "    return accu_feature\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "f1493414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 s, sys: 1.39 s, total: 30 s\n",
      "Wall time: 32.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cdtx_feature = accum_featuring(cdtxDate_df, 'date', 30)\n",
    "dp_feature = accum_featuring(dpDate_df, 'tx_date', 30)\n",
    "remit_feature = accum_featuring(remitDate_df, 'trans_date', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17acf1f9",
   "metadata": {},
   "source": [
    "**3. Integrate dataset**\n",
    "- moneyLundry dataset (sar_flag==1, in alert_date)\n",
    "- falseAlarm dataset (sar_flag==0, in alert_date)\n",
    "- public dataset (sar_flag==2, in alert_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "b8635dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat([custinfo_valid, cdtx_feature, dp_feature, remit_feature], axis=1)\n",
    "data_df = data_df[data_df['alert_key'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "664422ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sar_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>23451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sar_flag\n",
       "0.0     23451\n",
       "2.0      1770\n",
       "1.0       233"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelY = 'sar_flag'\n",
    "featureX = ['risk_rank', 'occupation_code', 'total_asset', 'AGE',\n",
    "       'cdtx_amt', 'cdtx_cnt', 'cdtx_amtNTD',\n",
    "       'cdtx_cntNTD', 'cdtx_amtFC', 'cdtx_cntFC', 'cdtx_amtTW', 'cdtx_cntTW',\n",
    "       'cdtx_amtF', 'cdtx_cntF', 'cdtx_amt_Accum', 'cdtx_cnt_Accum',\n",
    "       'cdtx_amtNTD_Accum', 'cdtx_cntNTD_Accum', 'cdtx_amtFC_Accum',\n",
    "       'cdtx_cntFC_Accum', 'cdtx_amtTW_Accum', 'cdtx_cntTW_Accum',\n",
    "       'cdtx_amtF_Accum', 'cdtx_cntF_Accum', 'dp_amt', 'dp_cnt', 'dp_amtCR',\n",
    "       'dp_cntCR', 'dp_amtDB', 'dp_cntDB', 'dp_amtCC', 'dp_cntCC', 'dp_amtNCC',\n",
    "       'dp_cntNCC', 'dp_amtCBank', 'dp_cntCBank', 'dp_amtInBank',\n",
    "       'dp_cntInBank', 'dp_amtATM', 'dp_cntATM', 'dp_amtNATM', 'dp_cntNATM',\n",
    "       'dp_branchNunique', 'dp_amt_Accum', 'dp_cnt_Accum', 'dp_amtCR_Accum',\n",
    "       'dp_cntCR_Accum', 'dp_amtDB_Accum', 'dp_cntDB_Accum', 'dp_amtCC_Accum',\n",
    "       'dp_cntCC_Accum', 'dp_amtNCC_Accum', 'dp_cntNCC_Accum',\n",
    "       'dp_amtCBank_Accum', 'dp_cntCBank_Accum', 'dp_amtInBank_Accum',\n",
    "       'dp_cntInBank_Accum', 'dp_amtATM_Accum', 'dp_cntATM_Accum',\n",
    "       'dp_amtNATM_Accum', 'dp_cntNATM_Accum', 'dp_branchNunique_Accum',\n",
    "       'remit_amt', 'remit_cnt', 'remit_amt_Accum', 'remit_cnt_Accum']\n",
    "\n",
    "data_df[labelY].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b480a9",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "8a9275d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import namedtuple\n",
    "SCORE_TUPLE = namedtuple('SCORE_TUPLE', 'model, iter, precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "734247a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_training(model, X_tr, y_tr, preprocess_scale=True, preprocess_pca=True):\n",
    "    if preprocess_scale:\n",
    "        scaler = MinMaxScaler().fit(X_tr)\n",
    "        X_tr = scaler.transform(X_tr)\n",
    "    else:\n",
    "        scaler=None\n",
    "        \n",
    "    if preprocess_pca:\n",
    "        pca = PCA(n_components=0.85)\n",
    "        pca.fit(X_trScaled)\n",
    "        X_tr = pca.transform(X_tr)\n",
    "    else:\n",
    "        pca=None\n",
    "    \n",
    "    X_re, y_re = SMOTE().fit_resample(X_tr, y_tr)\n",
    "    model = model.fit(X_re, y_re)\n",
    "    return model, scaler, pca \n",
    "\n",
    "def predict_prob(model, X_val, scaler, pca, preprocess_scale=True, preprocess_pca=True):\n",
    "    if preprocess_scale:\n",
    "        X_val = scaler.transform(X_val)\n",
    "    if preprocess_pca:\n",
    "        X_val = pca.transform(X_val)\n",
    "    y_predProb = model.predict_proba(X_val)\n",
    "    y_predProb = [x[1] for x in y_predProb]\n",
    "    return y_predProb\n",
    "\n",
    "def get_recallN_Precision(y_predProb, y_true):\n",
    "    y_pred_df = pd.DataFrame(list(zip(y_predProb, y_true)), columns=['predProb','trueLabel'])\n",
    "    y_pred_df.sort_values(by='predProb', ascending=False, inplace=True)\n",
    "    y_pred_df['idx']=list(range(len(y_pred_df)))\n",
    "    idx = y_pred_df[y_pred_df['trueLabel']==1]['idx'].iloc[-2]\n",
    "    precision_score = sum(y_true)/idx\n",
    "    return precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe575b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_Tr = data_df[data_df['sar_flag']!=2]\n",
    "dataset_TsPub = data_df[data_df['sar_flag']==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9a67a",
   "metadata": {},
   "source": [
    "**KFOLD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "52159b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:29] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:44:19] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:45:34] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:46:42] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CPU times: user 14min 48s, sys: 2min 11s, total: 17min\n",
      "Wall time: 4min 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayhsu/Library/Python/3.8/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preprocess_scale=True\n",
    "preprocess_pca=True\n",
    "X = dataset_Tr[featureX].fillna(0)\n",
    "Y = dataset_Tr[labelY]\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=23)\n",
    "skf.get_n_splits(X,Y)\n",
    "score_list = []\n",
    "\n",
    "for i, (tr_idx, val_idx) in enumerate(skf.split(X, Y)):\n",
    "    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = Y.iloc[tr_idx], Y.iloc[val_idx]\n",
    "    \n",
    "    modelLR = LogisticRegression(max_iter=300)\n",
    "    modelXGBC = XGBClassifier(use_label_encoder=False)\n",
    "    modelMLPC = MLPClassifier(max_iter=500)\n",
    "    \n",
    "    pred_prob_list = []\n",
    "    for model in [modelLR, modelXGBC, modelMLPC]:\n",
    "        model, scaler, pca = model_training(model, X_tr.copy(), y_tr.copy(), preprocess_scale, preprocess_pca)\n",
    "        y_predProb = predict_prob(model, X_val.copy(), scaler, pca, preprocess_scale, preprocess_pca)\n",
    "        precisionScore = get_recallN_Precision(y_predProb, y_val)\n",
    "        score_list.append(SCORE_TUPLE(model.__class__.__name__, i, precisionScore))\n",
    "        pred_prob_list.append(y_predProb)\n",
    "        \n",
    "    precisionScore = get_recallN_Precision(np.mean(pred_prob_list, axis=0), y_val)\n",
    "    score_list.append(SCORE_TUPLE('ensemble', i, precisionScore))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "c8c47bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.012175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>0.011573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.011467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble</th>\n",
       "      <td>0.014021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    precision\n",
       "model                        \n",
       "LogisticRegression   0.012175\n",
       "MLPClassifier        0.011573\n",
       "XGBClassifier        0.011467\n",
       "ensemble             0.014021"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>iter</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">LogisticRegression</th>\n",
       "      <th>0</th>\n",
       "      <td>0.012091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">MLPClassifier</th>\n",
       "      <th>0</th>\n",
       "      <td>0.010246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">XGBClassifier</th>\n",
       "      <th>0</th>\n",
       "      <td>0.010038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ensemble</th>\n",
       "      <th>0</th>\n",
       "      <td>0.011640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         precision\n",
       "model              iter           \n",
       "LogisticRegression 0      0.012091\n",
       "                   1      0.010779\n",
       "                   2      0.011444\n",
       "                   3      0.014387\n",
       "MLPClassifier      0      0.010246\n",
       "                   1      0.010567\n",
       "                   2      0.011522\n",
       "                   3      0.013958\n",
       "XGBClassifier      0      0.010038\n",
       "                   1      0.010833\n",
       "                   2      0.011350\n",
       "                   3      0.013648\n",
       "ensemble           0      0.011640\n",
       "                   1      0.010625\n",
       "                   2      0.011781\n",
       "                   3      0.022040"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iter</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.012091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>0.010246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.010038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble</th>\n",
       "      <td>0.011640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.010779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>0.010567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.010833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble</th>\n",
       "      <td>0.010625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2</th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.011444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>0.011522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.011350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble</th>\n",
       "      <td>0.011781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">3</th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.014387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>0.013958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.013648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensemble</th>\n",
       "      <td>0.022040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         precision\n",
       "iter model                        \n",
       "0    LogisticRegression   0.012091\n",
       "     MLPClassifier        0.010246\n",
       "     XGBClassifier        0.010038\n",
       "     ensemble             0.011640\n",
       "1    LogisticRegression   0.010779\n",
       "     MLPClassifier        0.010567\n",
       "     XGBClassifier        0.010833\n",
       "     ensemble             0.010625\n",
       "2    LogisticRegression   0.011444\n",
       "     MLPClassifier        0.011522\n",
       "     XGBClassifier        0.011350\n",
       "     ensemble             0.011781\n",
       "3    LogisticRegression   0.014387\n",
       "     MLPClassifier        0.013958\n",
       "     XGBClassifier        0.013648\n",
       "     ensemble             0.022040"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_df = pd.DataFrame(score_list)\n",
    "display(score_df.groupby('model')['precision'].mean().to_frame())\n",
    "display(score_df.groupby(['model','iter'])['precision'].max().to_frame())\n",
    "display(score_df.groupby(['iter','model'])['precision'].max().to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd5508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf4f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c8c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
